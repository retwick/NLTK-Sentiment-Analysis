{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retwick/NLTK-Sentiment-Analysis/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FChpVgHLqHCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a105a354-e08a-4589-f6bd-4472278c8b29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding, Flatten\n",
        "from keras.layers import Embedding, LSTM, BatchNormalization, Multiply, Permute, Dot\n",
        "from keras.layers import Dropout, Lambda, RepeatVector, multiply\n",
        "from keras.layers import Input, Activation, Bidirectional, GRU, Dense, CuDNNGRU, CuDNNLSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "# from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n",
        "np.random.seed(42)\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "tf.set_random_seed(1234)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
        "MAX_SEQUENCE_LENGTH = 100  # max input sequence length\n",
        "EMBEDDING_DIM = 300  # word embedding size\n",
        "\n",
        "\n",
        "def RNN_model(input_layer, num_class):  # RNN model \n",
        "    def smoothing_attention(x):\n",
        "        e = K.sigmoid(x)\n",
        "        s = K.sum(e, axis=-1, keepdims=True)\n",
        "        return e / s\n",
        "    def sharpening(x):\n",
        "        s = K.exp(x)\n",
        "        d = K.sum(s, axis=-1, keepdims=True)\n",
        "        return s/d\n",
        "    reg = 0.0001\n",
        "    dropout = 0.5\n",
        "    hidden_dim = 1024\n",
        "    vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False, dropout = 0.5))(input_layer)\n",
        "    lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(input_layer)\n",
        "    ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "    #     #ADD SHARPENING\n",
        "#     weights = Lambda(sharpening)(ee)\n",
        "    weights = Lambda(smoothing_attention)(ee)\n",
        "    weights = RepeatVector(2*hidden_dim)(weights)\n",
        "    weights = Permute([2, 1])(weights)   #transpose\n",
        "    \n",
        "    output = Multiply()([weights, lstm])\n",
        "    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n",
        "    output = Dense(512)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dense(256)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dropout(dropout)(output)\n",
        "    output = Dense(num_class, activation='softmax')(output)\n",
        "    model = Model(sequence_input, output)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H8BrfbQAfZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "021b4932-9967-4624-b2ee-e91a4c123e17"
      },
      "source": [
        "##Created a dictionary for word embeddings such that,\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "relation_id = {\n",
        "    'Cause-Effect(e1,e2)':0,\n",
        "    'Cause-Effect(e2,e1)':1,\n",
        "    'Content-Container(e1,e2)':2,\n",
        "    'Content-Container(e2,e1)':3,\n",
        "    'Component-Whole(e1,e2)':4,\n",
        "    'Component-Whole(e2,e1)':5,\n",
        "    'Entity-Origin(e1,e2)':6,\n",
        "    'Entity-Origin(e2,e1)':7,\n",
        "    'Entity-Destination(e1,e2)':8,\n",
        "    'Entity-Destination(e2,e1)':9,\n",
        "    'Instrument-Agency(e2,e1)':10,\n",
        "    'Instrument-Agency(e1,e2)':11,\n",
        "    'Message-Topic(e1,e2)':12,\n",
        "    'Message-Topic(e2,e1)':13,\n",
        "    'Member-Collection(e1,e2)':14,\n",
        "    'Member-Collection(e2,e1)':15,\n",
        "    'Product-Producer(e1,e2)':16,\n",
        "    'Product-Producer(e2,e1)':17,\n",
        "    'Other':18\n",
        "              }\n",
        "num_class = 19\n",
        "\n",
        "def id_to_relation(id):\n",
        "  for k in relation_id.keys():\n",
        "    if relation_id[k] == id:\n",
        "      return k\n",
        "  assert(False)\n",
        "  return -1\n",
        "\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TRAIN_FILE.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "new_lines = []\n",
        "for i in range(0, len(lines), 4):\n",
        "  sentence = lines[i].strip().split('\\t')[1][1:-1]\n",
        "  relation = lines[i+1].strip()\n",
        "  X_train.append(sentence)\n",
        "  assert(relation in relation_id)\n",
        "  Y_train.append(relation_id[relation])\n",
        "  \n",
        "Y_train = to_categorical(Y_train)\n",
        "\n",
        "print('Created X_train, Y_train')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created X_train, Y_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbVM4JChBeBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ed375b3-c838-4617-9747-5e47ad637705"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## TOKENIZE DATA POINTS\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)    #Fit on train data\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  #pad zeros at the beginning of each sequence to make all of the same length\n",
        "print('Tokenized.')\n",
        "# print(type(X_train))\n",
        "## CREATE VALIDATION SET\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "X_test = []\n",
        "ID_test = []\n",
        "print('Start test')\n",
        "##PREPROCESSING\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TEST_FILE.txt\") as f:\n",
        "    for l in f:\n",
        "        ID, sentence = l.strip().split(\"\\t\")\n",
        "        sentence = sentence[1:-1]\n",
        "        ID_test.append(ID)\n",
        "        X_test.append(sentence)\n",
        "##TOKENIZE TEST POINTS\n",
        "sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "## END TOKENIZE\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized.\n",
            "Start test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1felLqX4J8xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72be8caa-9a1b-45b1-ee52-5ce35b934f32"
      },
      "source": [
        "print('Start embedding')\n",
        "embeddings_index = {}\n",
        "with open('/gdrive/My Drive/NLP-Project/Dataset/glove.42B.300d.txt') as f:  # read pre-trained word embedding\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[values[0]] = coefs\n",
        "print('Created a dictionary for word embeddings')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start embedding\n",
            "Created a dictionary for word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTt_JlgXaOae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "082ecaa1-81a0-4f79-cced-890e5cdec40e"
      },
      "source": [
        "print('Create embedding weights')\n",
        "\n",
        "# print('Words not in glove:')\n",
        "word_index = tokenizer.word_index  # word dictionary <word, index>\n",
        "## CREATE EMBEDDING MATRIX\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # create word embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "#         print(word)   \n",
        "        pass\n",
        "\n",
        "print('')\n",
        "print('Stored embedding weights.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create embedding weights\n",
            "\n",
            "Stored embedding weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QKPmKHOqbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbn_8ivqWvK",
        "colab_type": "code",
        "outputId": "b5653f57-9a9e-4d48-d038-e1c8d18ebff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1581
        }
      },
      "source": [
        "## MAGIC BEGINS \n",
        "print('Magic Begins')\n",
        "\n",
        "##################################################################\n",
        "## USED EMBEDDING FROM KERAS\n",
        "embedding_layer = Embedding(num_words,  # inital word embedding weights\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')  # input layer\n",
        "embedded_sequences = embedding_layer(sequence_input)  # word embedding\n",
        "\n",
        "print('Call RNN')\n",
        "################################################\n",
        "# model = RNN_model(embedded_sequences, num_class)\n",
        "\n",
        "  \n",
        "dropout = 0.5\n",
        "hidden_dim = 1024\n",
        "\n",
        "vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False))(embedded_sequences)\n",
        "lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(embedded_sequences)\n",
        "ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "# Source: https://github.com/keras-team/keras/issues/4962#issuecomment-271934502\n",
        "weights = Activation('softmax')(ee)\n",
        "weights = RepeatVector(2*hidden_dim)(weights)\n",
        "weights = Permute([2, 1])(weights)  \n",
        "output = multiply([weights, lstm])\n",
        "output = Lambda(lambda x: K.sum(x, axis=-2))(output)\n",
        "# end of code from link\n",
        "\n",
        "output = Dense(512)(output)\n",
        "output = Dropout(dropout)(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Activation(\"relu\")(output)\n",
        "\n",
        "output = Dense(128)(output)\n",
        "# output = Dropout(dropout)(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Activation(\"relu\")(output)\n",
        "\n",
        "# output = Dropout(dropout)(output)\n",
        "output = Dense(num_class, activation='softmax')(output)\n",
        "\n",
        "model = Model(sequence_input, output)\n",
        "print(model.summary())\n",
        "\n",
        "# run RNN \n",
        "##################################################################\n",
        "\n",
        "print('Compiling Model')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.adam(lr= 0.001, amsgrad=True, clipvalue=10),\n",
        "            metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min')\n",
        "\n",
        "print('Start Fitting')\n",
        "model.fit(X_train, Y_train,\n",
        "        batch_size=200,\n",
        "        epochs=25,\n",
        "        callbacks=[early_stop],\n",
        "        validation_data=(X_val, Y_val))\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Magic Begins\n",
            "Call RNN\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_47 (InputLayer)           (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_51 (Embedding)        (None, 100, 300)     5865000     input_47[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_65 (Bidirectional (None, 2048)         8146944     embedding_51[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_66 (Bidirectional (None, 100, 2048)    8146944     embedding_51[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dot_32 (Dot)                    (None, 100)          0           bidirectional_65[0][0]           \n",
            "                                                                 bidirectional_66[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 100)          0           dot_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_30 (RepeatVector) (None, 2048, 100)    0           activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "permute_30 (Permute)            (None, 100, 2048)    0           repeat_vector_30[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "multiply_27 (Multiply)          (None, 100, 2048)    0           permute_30[0][0]                 \n",
            "                                                                 bidirectional_66[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 2048)         0           multiply_27[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_78 (Dense)                (None, 512)          1049088     lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 512)          0           dense_78[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 512)          2048        dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 512)          0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_79 (Dense)                (None, 128)          65664       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 128)          512         dense_79[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 128)          0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_80 (Dense)                (None, 19)           2451        activation_66[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 23,278,651\n",
            "Trainable params: 23,277,371\n",
            "Non-trainable params: 1,280\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Compiling Model\n",
            "Start Fitting\n",
            "Train on 7200 samples, validate on 800 samples\n",
            "Epoch 1/25\n",
            "7200/7200 [==============================] - 71s 10ms/step - loss: 2.1022 - acc: 0.3533 - val_loss: 1.5284 - val_acc: 0.5262\n",
            "Epoch 2/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 1.1587 - acc: 0.6306 - val_loss: 1.1447 - val_acc: 0.6462\n",
            "Epoch 3/25\n",
            "7200/7200 [==============================] - 58s 8ms/step - loss: 0.7667 - acc: 0.7614 - val_loss: 0.9204 - val_acc: 0.6963\n",
            "Epoch 4/25\n",
            "7200/7200 [==============================] - 58s 8ms/step - loss: 0.5116 - acc: 0.8446 - val_loss: 0.8208 - val_acc: 0.7350\n",
            "Epoch 5/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.3141 - acc: 0.9126 - val_loss: 0.8128 - val_acc: 0.7363\n",
            "Epoch 6/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.1734 - acc: 0.9608 - val_loss: 0.8774 - val_acc: 0.7450\n",
            "Epoch 7/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0915 - acc: 0.9819 - val_loss: 0.9330 - val_acc: 0.7600\n",
            "Epoch 8/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0505 - acc: 0.9935 - val_loss: 1.0726 - val_acc: 0.7338\n",
            "Epoch 9/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0278 - acc: 0.9971 - val_loss: 1.2172 - val_acc: 0.7188\n",
            "Epoch 10/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0160 - acc: 0.9985 - val_loss: 1.1790 - val_acc: 0.7375\n",
            "Epoch 11/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0100 - acc: 0.9996 - val_loss: 1.1644 - val_acc: 0.7412\n",
            "Epoch 12/25\n",
            "7200/7200 [==============================] - 57s 8ms/step - loss: 0.0066 - acc: 0.9999 - val_loss: 1.2839 - val_acc: 0.7262\n",
            "Epoch 13/25\n",
            "7200/7200 [==============================] - 59s 8ms/step - loss: 0.0050 - acc: 0.9997 - val_loss: 1.2367 - val_acc: 0.7375\n",
            "Epoch 14/25\n",
            "7200/7200 [==============================] - 53s 7ms/step - loss: 0.0037 - acc: 0.9999 - val_loss: 1.3057 - val_acc: 0.7363\n",
            "Epoch 15/25\n",
            "7200/7200 [==============================] - 53s 7ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 1.3248 - val_acc: 0.7388\n",
            "Epoch 16/25\n",
            "7200/7200 [==============================] - 53s 7ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 1.3114 - val_acc: 0.7438\n",
            "Epoch 17/25\n",
            "7200/7200 [==============================] - 52s 7ms/step - loss: 0.0025 - acc: 0.9999 - val_loss: 1.3130 - val_acc: 0.7438\n",
            "Epoch 18/25\n",
            "7200/7200 [==============================] - 53s 7ms/step - loss: 0.0023 - acc: 0.9999 - val_loss: 1.4035 - val_acc: 0.7388\n",
            "Epoch 19/25\n",
            "7200/7200 [==============================] - 52s 7ms/step - loss: 0.0023 - acc: 0.9999 - val_loss: 1.4453 - val_acc: 0.7338\n",
            "Epoch 20/25\n",
            "7200/7200 [==============================] - 52s 7ms/step - loss: 0.0019 - acc: 0.9999 - val_loss: 1.4531 - val_acc: 0.7300\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1c21558da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ4-P21uVrVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "683f10bc-7bbc-4e81-e52b-c43c45c04ea9"
      },
      "source": [
        "import time\n",
        "named_tuple = time.localtime() # get struct_time\n",
        "time_string = time.strftime(\"%m_%d_%H:%M:%S\", named_tuple)\n",
        "\n",
        "print('Saving to file ',time_string,'.txt')\n",
        "## fit model\n",
        "print('Start prediction')\n",
        "Y_pre = model.predict(X_test)\n",
        "# print(Y_pre[0])\n",
        "Y_pre = np.argmax(Y_pre, axis=1)\n",
        "Y_pre = [id_to_relation(i) for i in Y_pre]\n",
        "\n",
        "outfile = \"/gdrive/My Drive/NLP-Project/Output/\" + time_string + '.txt'\n",
        "with open(outfile, 'w') as f:\n",
        "    for ID, label in zip(ID_test, Y_pre):\n",
        "        f.write(ID + \"\\t\" + label + \"\\n\")\n",
        "## do prediction  \n",
        "print('Finished Prediction, Check result!')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving to file  05_06_13:03:32 .txt\n",
            "Start prediction\n",
            "Finished Prediction, Check result!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pihSHrz5l42G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}