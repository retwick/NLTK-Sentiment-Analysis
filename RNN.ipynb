{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retwick/NLTK-Sentiment-Analysis/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FChpVgHLqHCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4f1d2a1f-e84c-460a-bf92-de564dd6fa5f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding, Flatten\n",
        "from keras.layers import Embedding, LSTM, BatchNormalization, Multiply, Permute, Dot\n",
        "from keras.layers import Dropout, Lambda, RepeatVector, multiply\n",
        "from keras.layers import Input, Activation, Bidirectional, GRU, Dense, CuDNNGRU, CuDNNLSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "# from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n",
        "np.random.seed(42)\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "tf.set_random_seed(1234)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
        "MAX_SEQUENCE_LENGTH = 100  # max input sequence length\n",
        "EMBEDDING_DIM = 300  # word embedding size\n",
        "\n",
        "\n",
        "def RNN_model(input_layer, num_class):  # RNN model \n",
        "    def smoothing_attention(x):\n",
        "        e = K.sigmoid(x)\n",
        "        s = K.sum(e, axis=-1, keepdims=True)\n",
        "        return e / s\n",
        "    def sharpening(x):\n",
        "        s = K.exp(x)\n",
        "        d = K.sum(s, axis=-1, keepdims=True)\n",
        "        return s/d\n",
        "    reg = 0.0001\n",
        "    dropout = 0.5\n",
        "    hidden_dim = 1024\n",
        "    vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False, dropout = 0.5))(input_layer)\n",
        "    lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(input_layer)\n",
        "    ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "    #     #ADD SHARPENING\n",
        "#     weights = Lambda(sharpening)(ee)\n",
        "    weights = Lambda(smoothing_attention)(ee)\n",
        "    weights = RepeatVector(2*hidden_dim)(weights)\n",
        "    weights = Permute([2, 1])(weights)   #transpose\n",
        "    \n",
        "    output = Multiply()([weights, lstm])\n",
        "    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n",
        "    output = Dense(512)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dense(256)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dropout(dropout)(output)\n",
        "    output = Dense(num_class, activation='softmax')(output)\n",
        "    model = Model(sequence_input, output)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H8BrfbQAfZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "308c7267-a5c8-43a9-cfb2-253668ecef51"
      },
      "source": [
        "##Created a dictionary for word embeddings such that,\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "relation_id = {\n",
        "    'Cause-Effect(e1,e2)':0,\n",
        "    'Cause-Effect(e2,e1)':1,\n",
        "    'Content-Container(e1,e2)':2,\n",
        "    'Content-Container(e2,e1)':3,\n",
        "    'Component-Whole(e1,e2)':4,\n",
        "    'Component-Whole(e2,e1)':5,\n",
        "    'Entity-Origin(e1,e2)':6,\n",
        "    'Entity-Origin(e2,e1)':7,\n",
        "    'Entity-Destination(e1,e2)':8,\n",
        "    'Entity-Destination(e2,e1)':9,\n",
        "    'Instrument-Agency(e2,e1)':10,\n",
        "    'Instrument-Agency(e1,e2)':11,\n",
        "    'Message-Topic(e1,e2)':12,\n",
        "    'Message-Topic(e2,e1)':13,\n",
        "    'Member-Collection(e1,e2)':14,\n",
        "    'Member-Collection(e2,e1)':15,\n",
        "    'Product-Producer(e1,e2)':16,\n",
        "    'Product-Producer(e2,e1)':17,\n",
        "    'Other':18\n",
        "              }\n",
        "num_class = 19\n",
        "\n",
        "def id_to_relation(id):\n",
        "  for k in relation_id.keys():\n",
        "    if relation_id[k] == id:\n",
        "      return k\n",
        "  assert(False)\n",
        "  return -1\n",
        "\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TRAIN_FILE.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "new_lines = []\n",
        "for i in range(0, len(lines), 4):\n",
        "  sentence = lines[i].strip().split('\\t')[1][1:-1]\n",
        "  relation = lines[i+1].strip()\n",
        "  X_train.append(sentence)\n",
        "  assert(relation in relation_id)\n",
        "  Y_train.append(relation_id[relation])\n",
        "  \n",
        "Y_train = to_categorical(Y_train)\n",
        "\n",
        "print('Created X_train, Y_train')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created X_train, Y_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbVM4JChBeBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6115e4f8-39b0-438f-b737-dd2b9a0a4c60"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## TOKENIZE DATA POINTS\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)    #Fit on train data\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  #pad zeros at the beginning of each sequence to make all of the same length\n",
        "print('Tokenized.')\n",
        "# print(type(X_train))\n",
        "## CREATE VALIDATION SET\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "X_test = []\n",
        "ID_test = []\n",
        "print('Start test')\n",
        "##PREPROCESSING\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TEST_FILE.txt\") as f:\n",
        "    for l in f:\n",
        "        ID, sentence = l.strip().split(\"\\t\")\n",
        "        sentence = sentence[1:-1]\n",
        "        ID_test.append(ID)\n",
        "        X_test.append(sentence)\n",
        "##TOKENIZE TEST POINTS\n",
        "sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "## END TOKENIZE\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized.\n",
            "Start test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1felLqX4J8xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "70a21c4a-49a3-455b-92e7-c5528da4f034"
      },
      "source": [
        "print('Start embedding')\n",
        "embeddings_index = {}\n",
        "with open('/gdrive/My Drive/NLP-Project/Dataset/glove.42B.300d.txt') as f:  # read pre-trained word embedding\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[values[0]] = coefs\n",
        "print('Created a dictionary for word embeddings')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start embedding\n",
            "Created a dictionary for word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTt_JlgXaOae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "94794b2d-650c-4bd9-d051-fec4a6f5d3d4"
      },
      "source": [
        "print('Create embedding weights')\n",
        "\n",
        "# print('Words not in glove:')\n",
        "word_index = tokenizer.word_index  # word dictionary <word, index>\n",
        "## CREATE EMBEDDING MATRIX\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # create word embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "#         print(word)   \n",
        "        pass\n",
        "\n",
        "print('')\n",
        "print('Stored embedding weights.')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create embedding weights\n",
            "\n",
            "Stored embedding weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QKPmKHOqbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbn_8ivqWvK",
        "colab_type": "code",
        "outputId": "d6c7eb58-4138-4c41-b45b-995943a52cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "source": [
        "## MAGIC BEGINS \n",
        "print('Magic Begins')\n",
        "\n",
        "##################################################################\n",
        "## USED EMBEDDING FROM KERAS\n",
        "embedding_layer = Embedding(num_words,  # inital word embedding weights\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')  # input layer\n",
        "embedded_sequences = embedding_layer(sequence_input)  # word embedding\n",
        "\n",
        "print('Call RNN')\n",
        "################################################\n",
        "# model = RNN_model(embedded_sequences, num_class)\n",
        "\n",
        "  \n",
        "dropout = 0.5\n",
        "hidden_dim = 512\n",
        "\n",
        "vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False))(embedded_sequences)\n",
        "lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(embedded_sequences)\n",
        "ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "# Source: https://github.com/keras-team/keras/issues/4962#issuecomment-271934502\n",
        "weights = Activation('softmax')(ee)    \n",
        "weights = RepeatVector(2*hidden_dim)(weights) \n",
        "weights = Permute([2, 1])(weights)  \n",
        "output = multiply([weights, lstm])\n",
        "output = Lambda(lambda x: K.sum(x, axis=-2))(output)\n",
        "# end of code from link\n",
        "\n",
        "output = Dense(256)(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Activation(\"relu\")(output)\n",
        "output = Dropout(dropout)(output)\n",
        "\n",
        "output = Dense(128)(output)\n",
        "output = BatchNormalization()(output)\n",
        "output = Activation(\"relu\")(output)\n",
        "\n",
        "# output = Dense(32)(output)\n",
        "# output = BatchNormalization()(output)\n",
        "# output = Activation(\"relu\")(output)\n",
        "\n",
        "# output = Dropout(dropout)(output)\n",
        "output = Dense(num_class, activation='softmax')(output)\n",
        "\n",
        "model = Model(sequence_input, output)\n",
        "print(model.summary())\n",
        "\n",
        "# run RNN \n",
        "##################################################################\n",
        "\n",
        "print('Compiling Model')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.adam(lr= 0.001, amsgrad=True, clipvalue=10),\n",
        "            metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min')\n",
        "\n",
        "print('Start Fitting')\n",
        "model.fit(X_train, Y_train,\n",
        "        batch_size=200,\n",
        "        epochs=25,\n",
        "        callbacks=[early_stop],\n",
        "        validation_data=(X_val, Y_val))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Magic Begins\n",
            "Call RNN\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, 100, 300)     5865000     input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 1024)         2500608     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_6 (Bidirectional) (None, 100, 1024)    2500608     embedding_3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 100)          0           bidirectional_5[0][0]            \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 100)          0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 1024, 100)    0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 100, 1024)    0           repeat_vector_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 100, 1024)    0           permute_3[0][0]                  \n",
            "                                                                 bidirectional_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 1024)         0           multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 256)          262400      lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 256)          1024        dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 256)          0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 256)          0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_8 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 128)          512         dense_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 128)          0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 19)           2451        activation_9[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 11,165,499\n",
            "Trainable params: 11,164,731\n",
            "Non-trainable params: 768\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Compiling Model\n",
            "Start Fitting\n",
            "Train on 7200 samples, validate on 800 samples\n",
            "Epoch 1/25\n",
            "7200/7200 [==============================] - 23s 3ms/step - loss: 2.1971 - acc: 0.3525 - val_loss: 1.6867 - val_acc: 0.4987\n",
            "Epoch 2/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 1.2397 - acc: 0.6249 - val_loss: 1.1433 - val_acc: 0.6487\n",
            "Epoch 3/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.8606 - acc: 0.7344 - val_loss: 1.1699 - val_acc: 0.6450\n",
            "Epoch 4/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.6046 - acc: 0.8153 - val_loss: 0.9412 - val_acc: 0.7212\n",
            "Epoch 5/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.3916 - acc: 0.8935 - val_loss: 0.9852 - val_acc: 0.6962\n",
            "Epoch 6/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.2454 - acc: 0.9392 - val_loss: 1.2301 - val_acc: 0.6487\n",
            "Epoch 7/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.1390 - acc: 0.9717 - val_loss: 1.0374 - val_acc: 0.7212\n",
            "Epoch 8/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0827 - acc: 0.9856 - val_loss: 1.1652 - val_acc: 0.6887\n",
            "Epoch 9/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0540 - acc: 0.9926 - val_loss: 1.1073 - val_acc: 0.7212\n",
            "Epoch 10/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0316 - acc: 0.9975 - val_loss: 1.0759 - val_acc: 0.7300\n",
            "Epoch 11/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0230 - acc: 0.9983 - val_loss: 1.1087 - val_acc: 0.7250\n",
            "Epoch 12/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0176 - acc: 0.9989 - val_loss: 1.1090 - val_acc: 0.7288\n",
            "Epoch 13/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0146 - acc: 0.9993 - val_loss: 1.1186 - val_acc: 0.7300\n",
            "Epoch 14/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0119 - acc: 0.9992 - val_loss: 1.1141 - val_acc: 0.7375\n",
            "Epoch 15/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0086 - acc: 0.9994 - val_loss: 1.1753 - val_acc: 0.7275\n",
            "Epoch 16/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0072 - acc: 0.9999 - val_loss: 1.1684 - val_acc: 0.7375\n",
            "Epoch 17/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0058 - acc: 0.9997 - val_loss: 1.1579 - val_acc: 0.7375\n",
            "Epoch 18/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0057 - acc: 0.9999 - val_loss: 1.2365 - val_acc: 0.7262\n",
            "Epoch 19/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0045 - acc: 0.9999 - val_loss: 1.1740 - val_acc: 0.7438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd521421a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ4-P21uVrVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e2305805-c823-4f59-e4b0-f7e64fbb4c66"
      },
      "source": [
        "import time\n",
        "named_tuple = time.localtime() # get struct_time\n",
        "time_string = time.strftime(\"%m_%d_%H:%M:%S\", named_tuple)\n",
        "\n",
        "print('Saving to file ',time_string,'.txt')\n",
        "## fit model\n",
        "print('Start prediction')\n",
        "Y_pre = model.predict(X_test)\n",
        "# print(Y_pre[0])\n",
        "Y_pre = np.argmax(Y_pre, axis=1)\n",
        "Y_pre = [id_to_relation(i) for i in Y_pre]\n",
        "\n",
        "outfile = \"/gdrive/My Drive/NLP-Project/Output/\" + time_string + '.txt'\n",
        "with open(outfile, 'w') as f:\n",
        "    for ID, label in zip(ID_test, Y_pre):\n",
        "        f.write(ID + \"\\t\" + label + \"\\n\")\n",
        "## do prediction  \n",
        "print('Finished Prediction, Check result!')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving to file  05_06_13:56:12 .txt\n",
            "Start prediction\n",
            "Finished Prediction, Check result!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pihSHrz5l42G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}