{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/retwick/NLTK-Sentiment-Analysis/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FChpVgHLqHCj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a105a354-e08a-4589-f6bd-4472278c8b29"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Masking, Embedding, Flatten\n",
        "from keras.layers import Embedding, LSTM, BatchNormalization, Multiply, Permute, Dot\n",
        "from keras.layers import Dropout, Lambda, RepeatVector, multiply\n",
        "from keras.layers import Input, Activation, Bidirectional, GRU, Dense, CuDNNGRU, CuDNNLSTM\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import backend as K\n",
        "# from sklearn.metrics import f1_score, confusion_matrix, mean_squared_error\n",
        "np.random.seed(42)\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "tf.set_random_seed(1234)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)\n",
        "\n",
        "\n",
        "BASE_DIR = ''\n",
        "GLOVE_DIR = os.path.join(BASE_DIR, '')\n",
        "MAX_SEQUENCE_LENGTH = 100  # max input sequence length\n",
        "EMBEDDING_DIM = 300  # word embedding size\n",
        "\n",
        "\n",
        "def RNN_model(input_layer, num_class):  # RNN model \n",
        "    def smoothing_attention(x):\n",
        "        e = K.sigmoid(x)\n",
        "        s = K.sum(e, axis=-1, keepdims=True)\n",
        "        return e / s\n",
        "    def sharpening(x):\n",
        "        s = K.exp(x)\n",
        "        d = K.sum(s, axis=-1, keepdims=True)\n",
        "        return s/d\n",
        "    reg = 0.0001\n",
        "    dropout = 0.5\n",
        "    hidden_dim = 1024\n",
        "    vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False, dropout = 0.5))(input_layer)\n",
        "    lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(input_layer)\n",
        "    ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "    #     #ADD SHARPENING\n",
        "#     weights = Lambda(sharpening)(ee)\n",
        "    weights = Lambda(smoothing_attention)(ee)\n",
        "    weights = RepeatVector(2*hidden_dim)(weights)\n",
        "    weights = Permute([2, 1])(weights)   #transpose\n",
        "    \n",
        "    output = Multiply()([weights, lstm])\n",
        "    output = Lambda(lambda x: K.sum(x, axis=1))(output)\n",
        "    output = Dense(512)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dense(256)(output)\n",
        "    output = BatchNormalization()(output)\n",
        "    output = Activation(\"relu\")(output)\n",
        "    output = Dropout(dropout)(output)\n",
        "    output = Dense(num_class, activation='softmax')(output)\n",
        "    model = Model(sequence_input, output)\n",
        "    print(model.summary())\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9H8BrfbQAfZW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "021b4932-9967-4624-b2ee-e91a4c123e17"
      },
      "source": [
        "##Created a dictionary for word embeddings such that,\n",
        "\n",
        "X_train = []\n",
        "Y_train = []\n",
        "relation_id = {\n",
        "    'Cause-Effect(e1,e2)':0,\n",
        "    'Cause-Effect(e2,e1)':1,\n",
        "    'Content-Container(e1,e2)':2,\n",
        "    'Content-Container(e2,e1)':3,\n",
        "    'Component-Whole(e1,e2)':4,\n",
        "    'Component-Whole(e2,e1)':5,\n",
        "    'Entity-Origin(e1,e2)':6,\n",
        "    'Entity-Origin(e2,e1)':7,\n",
        "    'Entity-Destination(e1,e2)':8,\n",
        "    'Entity-Destination(e2,e1)':9,\n",
        "    'Instrument-Agency(e2,e1)':10,\n",
        "    'Instrument-Agency(e1,e2)':11,\n",
        "    'Message-Topic(e1,e2)':12,\n",
        "    'Message-Topic(e2,e1)':13,\n",
        "    'Member-Collection(e1,e2)':14,\n",
        "    'Member-Collection(e2,e1)':15,\n",
        "    'Product-Producer(e1,e2)':16,\n",
        "    'Product-Producer(e2,e1)':17,\n",
        "    'Other':18\n",
        "              }\n",
        "num_class = 19\n",
        "\n",
        "def id_to_relation(id):\n",
        "  for k in relation_id.keys():\n",
        "    if relation_id[k] == id:\n",
        "      return k\n",
        "  assert(False)\n",
        "  return -1\n",
        "\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TRAIN_FILE.txt\") as f:\n",
        "  lines = f.readlines()\n",
        "new_lines = []\n",
        "for i in range(0, len(lines), 4):\n",
        "  sentence = lines[i].strip().split('\\t')[1][1:-1]\n",
        "  relation = lines[i+1].strip()\n",
        "  X_train.append(sentence)\n",
        "  assert(relation in relation_id)\n",
        "  Y_train.append(relation_id[relation])\n",
        "  \n",
        "Y_train = to_categorical(Y_train)\n",
        "\n",
        "print('Created X_train, Y_train')\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created X_train, Y_train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbVM4JChBeBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7ed375b3-c838-4617-9747-5e47ad637705"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "## TOKENIZE DATA POINTS\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)    #Fit on train data\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  #pad zeros at the beginning of each sequence to make all of the same length\n",
        "print('Tokenized.')\n",
        "# print(type(X_train))\n",
        "## CREATE VALIDATION SET\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "X_test = []\n",
        "ID_test = []\n",
        "print('Start test')\n",
        "##PREPROCESSING\n",
        "with open(\"/gdrive/My Drive/NLP-Project/Dataset/TEST_FILE.txt\") as f:\n",
        "    for l in f:\n",
        "        ID, sentence = l.strip().split(\"\\t\")\n",
        "        sentence = sentence[1:-1]\n",
        "        ID_test.append(ID)\n",
        "        X_test.append(sentence)\n",
        "##TOKENIZE TEST POINTS\n",
        "sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "## END TOKENIZE\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized.\n",
            "Start test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1felLqX4J8xq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "72be8caa-9a1b-45b1-ee52-5ce35b934f32"
      },
      "source": [
        "print('Start embedding')\n",
        "embeddings_index = {}\n",
        "with open('/gdrive/My Drive/NLP-Project/Dataset/glove.42B.300d.txt') as f:  # read pre-trained word embedding\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[values[0]] = coefs\n",
        "print('Created a dictionary for word embeddings')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start embedding\n",
            "Created a dictionary for word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTt_JlgXaOae",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "082ecaa1-81a0-4f79-cced-890e5cdec40e"
      },
      "source": [
        "print('Create embedding weights')\n",
        "\n",
        "# print('Words not in glove:')\n",
        "word_index = tokenizer.word_index  # word dictionary <word, index>\n",
        "## CREATE EMBEDDING MATRIX\n",
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))  # create word embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in embeddings_index.keys():\n",
        "        embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "#         print(word)   \n",
        "        pass\n",
        "\n",
        "print('')\n",
        "print('Stored embedding weights.')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create embedding weights\n",
            "\n",
            "Stored embedding weights.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31QKPmKHOqbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORbn_8ivqWvK",
        "colab_type": "code",
        "outputId": "cfe70df8-344d-4ebe-e3eb-83819b2dc25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1309
        }
      },
      "source": [
        "## MAGIC BEGINS \n",
        "print('Magic Begins')\n",
        "reg = 0.0001\n",
        "dropout = 0.5\n",
        "units = 512\n",
        "\n",
        "##################################################################\n",
        "## USED EMBEDDING FROM KERAS\n",
        "embedding_layer = Embedding(num_words,  # inital word embedding weights\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=True)\n",
        "\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')  # input layer\n",
        "embedded_sequences = embedding_layer(sequence_input)  # word embedding\n",
        "\n",
        "print('Call RNN')\n",
        "################################################\n",
        "# model = RNN_model(embedded_sequences, num_class)\n",
        "\n",
        "def smoothing_attention(x):\n",
        "    e = K.sigmoid(x)\n",
        "    s = K.sum(e, axis=-1, keepdims=True)\n",
        "    return e / s\n",
        "  \n",
        "# reg = 0.0001\n",
        "dropout = 0.5\n",
        "hidden_dim = 512\n",
        "vector = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=False))(embedded_sequences)\n",
        "lstm = Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))(embedded_sequences)\n",
        "\n",
        "ee = Dot(axes=-1, normalize=True)([vector, lstm])  # calculate cosine similarity\n",
        "\n",
        "# Source: https://github.com/keras-team/keras/issues/4962#issuecomment-271934502\n",
        "weights = Activation('softmax')(ee)\n",
        "weights = RepeatVector(2*hidden_dim)(weights)\n",
        "weights = Permute([2, 1])(weights)  \n",
        "output = multiply([weights, lstm])\n",
        "output = Lambda(lambda x: K.sum(x, axis=-2))(output)\n",
        "# end of code from link\n",
        "\n",
        "# output = Dense(256)(output)\n",
        "# output = BatchNormalization()(output)\n",
        "# output = Activation(\"relu\")(output)\n",
        "# output = Dense(128)(output)\n",
        "output = BatchNormalization()(output)\n",
        "# output = Activation(\"relu\")(output)\n",
        "# output = Dropout(dropout)(output)\n",
        "output = Dense(num_class, activation='softmax')(output)\n",
        "model = Model(sequence_input, output)\n",
        "print(model.summary())\n",
        "\n",
        "# run RNN \n",
        "##################################################################\n",
        "\n",
        "print('Compiling Model')\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "            optimizer=keras.optimizers.adam(lr= 0.001, amsgrad=True, clipvalue=10),\n",
        "            metrics=['accuracy'])\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min')\n",
        "\n",
        "print('Start Fitting')\n",
        "model.fit(X_train, Y_train,\n",
        "        batch_size=200,\n",
        "        epochs=25,\n",
        "        callbacks=[early_stop],\n",
        "        validation_data=(X_val, Y_val))\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Magic Begins\n",
            "Call RNN\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_40 (InputLayer)           (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_44 (Embedding)        (None, 100, 300)     5865000     input_40[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_51 (Bidirectional (None, 1024)         2500608     embedding_44[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_52 (Bidirectional (None, 100, 1024)    2500608     embedding_44[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dot_25 (Dot)                    (None, 100)          0           bidirectional_51[0][0]           \n",
            "                                                                 bidirectional_52[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 100)          0           dot_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_23 (RepeatVector) (None, 1024, 100)    0           activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "permute_23 (Permute)            (None, 100, 1024)    0           repeat_vector_23[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "multiply_20 (Multiply)          (None, 100, 1024)    0           permute_23[0][0]                 \n",
            "                                                                 bidirectional_52[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 1024)         0           multiply_20[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 1024)         4096        lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_60 (Dense)                (None, 19)           19475       batch_normalization_36[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 10,889,787\n",
            "Trainable params: 10,887,739\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Compiling Model\n",
            "Start Fitting\n",
            "Train on 7200 samples, validate on 800 samples\n",
            "Epoch 1/25\n",
            "7200/7200 [==============================] - 28s 4ms/step - loss: 1.7400 - acc: 0.5004 - val_loss: 1.5867 - val_acc: 0.4913\n",
            "Epoch 2/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.8476 - acc: 0.7467 - val_loss: 1.1810 - val_acc: 0.6225\n",
            "Epoch 3/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.4673 - acc: 0.8560 - val_loss: 1.0401 - val_acc: 0.6800\n",
            "Epoch 4/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.2053 - acc: 0.9410 - val_loss: 1.3169 - val_acc: 0.7100\n",
            "Epoch 5/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0756 - acc: 0.9808 - val_loss: 1.5448 - val_acc: 0.7013\n",
            "Epoch 6/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0252 - acc: 0.9958 - val_loss: 1.5349 - val_acc: 0.7125\n",
            "Epoch 7/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0095 - acc: 0.9992 - val_loss: 1.4965 - val_acc: 0.7175\n",
            "Epoch 8/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0059 - acc: 0.9994 - val_loss: 1.5815 - val_acc: 0.7162\n",
            "Epoch 9/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.4917 - val_acc: 0.7225\n",
            "Epoch 10/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 1.4994 - val_acc: 0.7200\n",
            "Epoch 11/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 1.5116 - val_acc: 0.7250\n",
            "Epoch 12/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 8.6283e-04 - acc: 1.0000 - val_loss: 1.5239 - val_acc: 0.7275\n",
            "Epoch 13/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 7.2829e-04 - acc: 1.0000 - val_loss: 1.5367 - val_acc: 0.7262\n",
            "Epoch 14/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 6.3966e-04 - acc: 1.0000 - val_loss: 1.5413 - val_acc: 0.7262\n",
            "Epoch 15/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 5.4705e-04 - acc: 1.0000 - val_loss: 1.5488 - val_acc: 0.7262\n",
            "Epoch 16/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 5.0053e-04 - acc: 1.0000 - val_loss: 1.5555 - val_acc: 0.7262\n",
            "Epoch 17/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 4.5549e-04 - acc: 1.0000 - val_loss: 1.5721 - val_acc: 0.7250\n",
            "Epoch 18/25\n",
            "7200/7200 [==============================] - 20s 3ms/step - loss: 4.1743e-04 - acc: 1.0000 - val_loss: 1.5793 - val_acc: 0.7262\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1c2b906940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ4-P21uVrVN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f825dd2b-1ff2-4635-87f1-ff4ca93b7770"
      },
      "source": [
        "import time\n",
        "named_tuple = time.localtime() # get struct_time\n",
        "time_string = time.strftime(\"%m_%d_%H:%M:%S\", named_tuple)\n",
        "\n",
        "print('Saving to file ',time_string,'.txt')\n",
        "## fit model\n",
        "print('Start prediction')\n",
        "Y_pre = model.predict(X_test)\n",
        "# print(Y_pre[0])\n",
        "Y_pre = np.argmax(Y_pre, axis=1)\n",
        "Y_pre = [id_to_relation(i) for i in Y_pre]\n",
        "\n",
        "outfile = \"/gdrive/My Drive/NLP-Project/Output/\" + time_string + '.txt'\n",
        "with open(outfile, 'w') as f:\n",
        "    for ID, label in zip(ID_test, Y_pre):\n",
        "        f.write(ID + \"\\t\" + label + \"\\n\")\n",
        "## do prediction  \n",
        "print('Finished Prediction, Check result!')"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving to file  05_06_11:37:24 .txt\n",
            "Start prediction\n",
            "Finished Prediction, Check result!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pihSHrz5l42G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}